---
layout: post
title: Blog Post 2: Spectral Clustering
---

> In this blog post, I'll write a tutorial on a simple version of the *spectral clustering* algorithm for clustering data points. Each of the below parts are necessary specific tasks for creating the spectral clustering.

### Notation

Before starting doing some linear algebra in python, here are some specified notations used in this blog post:

- Boldface capital letters like **A** refer to matrices (2d arrays of numbers). 
- Boldface lowercase letters like **v** refer to vectors (1d arrays of numbers). 
- **AB** refers to a matrix-matrix product (`A@B`). **Av** refers to a matrix-vector product (`A@v`).

## Introduction

This part is the introduction of studying *spectral clustering*. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure. To start, let's look at an example where we *don't* need spectral clustering.

```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```
```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```
BP2_intro_1.png

*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these:

```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
BP2_intro_2.png


## Harder Clustering

That was all well and good, but what if our data is "shaped weird"? Here, we can use the function `make_moons` supported by python to make the moonlike clusters:

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```
BP2_harder_clustering_1.png

Similar as in *Intro*, can still make out two meaningful clusters in the data, but now they aren't blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. 

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
BP2_harder_clustering_2.png

Clearly from the figure above, the clustering went wrong with the k-Means algorithm.

As will suggest below, spectral clustering is able to correctly cluster the two crescents. The following parts shows how to derive and implement spectral clustering.

## Part A: Similarity Matrix Construction

> In this part, construct a *similarity matrix* **A**, a matrix (2d `np.ndarray`) with shape `(n, n)` (recall that `n` is the number of data points), 

- The matrix construction should be based on another external parameter `epsilon`. Specifically, for each entry `A[i,j]` of **A**, the value should be equal to one if data point `i` is within the distance `epsilon` of data point j, which is to say, \[A[i, j] = 1 \text{ if } (X[i] - X[j])**2 < epsilon**2\]

- Moreover, the diagonal entries `A[i,i]` should all be equal to zero.

For this part, I specified the value `epsilon = 0.4`. Also, to be more convenient for function construction, I used the function `pairwise_distance` from `sklearn.metrics` module, which could compute the euclidean distance between specified data points i and j.

```python
epsilon = 0.4
from sklearn.metrics import pairwise_distances
```

To construct the similarity matrix **A**, I construct a function `similarity_matrix_construction`, which inputs the set of data points `X` and the `epsilon` value. Below is the function def.

```python
def similarity_matrix_construction(X, epsilon):
  """
  Construct a similarity matrix indicator for whether each X[i]
  and X[j] are less than epsilon apart. Also, diagonal entries
  should all be zero for the desired output matrix.

  Input
  --------
  X: all data points informations
  epsilon: the maximum distance to be accepted

  Output
  --------
  A: the desired similarity matrix
  """
  # get dimension
  n = X.shape[0]
  # create empty matrix
  A = np.ndarray(shape = (n, n))
  # create matrix of distances using pairwise_distances
  # note: pairwise distances create a matrix D with D_ij as
  # distance between point X[i] and X[j]
  D = pairwise_distances(X, metric = 'euclidean')
  # now try to fill in A with information in D and epsilon
  A[D < epsilon**2] = 1
  A[D >= epsilon**2] = 0
  # empty the diagonal entries
  np.fill_diagonal(A, 0)
  # return similarity matrix
  return A
```

Now, store such similarity matrix as **A** for further use, and visualize the matrix:

```python
A = similarity_matrix_construction(X, epsilon)
A
```
```text
array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 1.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 1., 0., 0.]])
```

{::options parse_block_html="true" /}
<div class="got-help">
I learned something really cool from my peer feedback! 
</div>
{::options parse_block_html="false" /}

{::options parse_block_html="true" /}
<div class="gave-help">
I gave one of my peers a cool suggestion! 
</div>
{::options parse_block_html="false" /}