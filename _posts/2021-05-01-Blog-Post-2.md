---
layout: post
title: Blog Post 2 - Spectral Clustering
---

> In this blog post, I'll write a tutorial on a simple version of the *spectral clustering* algorithm for clustering data points. Each of the below parts are necessary specific tasks for creating the spectral clustering.

## Introduction

This part is the introduction of studying *spectral clustering*. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure. To start, let's look at an example where we *don't* need spectral clustering.

Note: This part is referenced from Phil's PIC 16B Blog Post 2 Spectral Clustering introduction part.

```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```
```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```
![BP2_intro_1.png]({{ site.baseurl }}/images/BP2_intro_1.png)

*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these:

```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![BP2_intro_2.png]({{ site.baseurl }}/images/BP2_intro_2.png)


## Harder Clustering

Note: This part is referenced from Phil's PIC 16B Blog Post 2 Spectral Clustering introduction part.

That was all well and good, but what if our data is "shaped weird"? Here, we can use the function `make_moons` supported by python to make the moonlike clusters:

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```
![BP2_harder_clustering_1.png]({{ site.baseurl }}/images/BP2_harder_clustering_1.png)

Similar as in *Intro*, can still make out two meaningful clusters in the data, but now they aren't blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. 

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![BP2_harder_clustering_2.png]({{ site.baseurl }}/images/BP2_harder_clustering_2.png)

Clearly from the figure above, the clustering went wrong with the k-Means algorithm.

As will suggest below, spectral clustering is able to correctly cluster the two crescents. The following parts shows how to derive and implement spectral clustering.

## Part A: Similarity Matrix Construction

> In this part, construct an <img src="https://render.githubusercontent.com/render/math?math=n \times n"> *similarity matrix* **A**, where n is the number of data points in the created cluster plot

- The matrix construction should be based on another external parameter `epsilon`. Specifically, for each entry `A[i,j]` of **A**, the value should be equal to one if data point `i` is within the distance `epsilon` of data point j, which is to say,

$$A[i, j] = 1 \text{ if } (X[i] - X[j])**2 < epsilon**2$$

- Moreover, the diagonal entries `A[i,i]` should all be equal to zero.

For this part, I specified the value `epsilon = 0.4`. Also, to be more convenient for function construction, I used the function `pairwise_distance` from `sklearn.metrics` module, which could compute the euclidean distance between specified data points i and j.

```python
epsilon = 0.4
from sklearn.metrics import pairwise_distances
```

To construct the similarity matrix **A**, I construct a function `similarity_matrix_construction`, which inputs the set of data points `X` and the `epsilon` value. Below is the function def.

```python
def similarity_matrix_construction(X, epsilon):
  """
  Construct a similarity matrix indicator for whether each X[i]
  and X[j] are less than epsilon apart. Also, diagonal entries
  should all be zero for the desired output matrix.

  Input
  --------
  X: all data points informations
  epsilon: the maximum distance to be accepted

  Output
  --------
  A: the desired similarity matrix
  """
  # get dimension
  n = X.shape[0]
  # create empty matrix
  A = np.ndarray(shape = (n, n))
  # create matrix of distances using pairwise_distances
  # note: pairwise distances create a matrix D with D_ij as
  # distance between point X[i] and X[j]
  D = pairwise_distances(X, metric = 'euclidean')
  # now try to fill in A with information in D and epsilon
  A[D < epsilon**2] = 1
  A[D >= epsilon**2] = 0
  # empty the diagonal entries
  np.fill_diagonal(A, 0)
  # return similarity matrix
  return A
```

Now, store such similarity matrix as **A** for further use, and visualize the matrix:

```python
A = similarity_matrix_construction(X, epsilon)
A
```
```text
array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 1.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 1., 0., 0.]])
```

The matrix **A** now contains information about which points are near (within distance `epsilon`) which other points.

## Part B: Clustering Data Points - Binary Norm Cut Objective of Similarity Matrix

> This part pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of **A**. 

### B.0 Introduce the Concept of Binary Norm Cut Objective

Let <img src="https://render.githubusercontent.com/render/math?math=d_i = \sum_{j = 1}^n a_{ij}"> be the `i`-th row-sum of **A**, which is also called the *degree* of `i`. Let <img src="https://render.githubusercontent.com/render/math?math=C_0"> and <img src="https://render.githubusercontent.com/render/math?math=C_1"> be two clusters of the data points. Assume that every data point is in either <img src="https://render.githubusercontent.com/render/math?math=C_0"> or <img src="https://render.githubusercontent.com/render/math?math=C_1">. The cluster membership as being specified by `y`. Think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row `i` of **A**) is an element of cluster <img src="https://render.githubusercontent.com/render/math?math=C_1">.  

The *binary norm cut objective* of a matrix **A** is the function 

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

In this expression, 
- <img src="https://render.githubusercontent.com/render/math?math=\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}"> is the *cut* of the clusters <img src="https://render.githubusercontent.com/render/math?math=C_0"> and <img src="https://render.githubusercontent.com/render/math?math=C_1">. 
- <img src="https://render.githubusercontent.com/render/math?math=\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$, where $d_i = \sum_{j = 1}^n a_{ij}"> is the *degree* of row `i` (the total number of all other rows related to row `i` through **A**). The *volume* of cluster <img src="https://render.githubusercontent.com/render/math?math=C_0"> is a measure of the size of the cluster. 

A pair of clusters <img src="https://render.githubusercontent.com/render/math?math=C_0"> and <img src="https://render.githubusercontent.com/render/math?math=C_1"> is considered to be a "good" partition of the data when <img src="https://render.githubusercontent.com/render/math?math=N_{\mathbf{A}}(C_0, C_1)"> is small. To see why, let's look at each of the two factors in this objective function separately.


### B.1 The Cut Term

> In this part, I'll write a function called `cut(A,y)` to compute the cut term.

The cut term <img src="https://render.githubusercontent.com/render/math?math=\mathbf{cut}(C_0, C_1)"> is the number of nonzero entries in **A** that relate points in cluster <img src="https://render.githubusercontent.com/render/math?math=C_0"> to points in cluster <img src="https://render.githubusercontent.com/render/math?math=C_1">. Saying that this term should be small is the same as saying that points in <img src="https://render.githubusercontent.com/render/math?math=C_0"> shouldn't usually be very close to points in <img src="https://render.githubusercontent.com/render/math?math=C_1">.

```python
def cut(A, y):
  """
  calculate cut term with sim matrix A and
  the cluster-specifying vector y.

  Input
  --------
  A: the similarity matrix created
  y: the cluster-specifying vector, 0 or 1 valued entries
  
  Output
  --------
  cut: the cut term
  """
  # create the two clusters
  C0 = np.where(y == 0)[0]
  C1 = np.where(y == 1)[0]
  # calculate cut sum
  cut = 0
  for i in C0:
    for j in C1:
      cut += A[i, j]
  return cut
```

Below is an example calculation of the cut objectives. The true clusters refers `y` created in the cluster creation process, and the random clusters refers a vector named `random`, which has the same objective as `y` except that it is created totally at random.

```python
# cut with true cluster y
true_cluster_cut = cut(A, y)
# cut with random cluster vector
np.random.seed(9999)
random = np.random.randint(2, size = A.shape[0])
random_cluster_cut = cut(A, random)
# find the two values with print
print("The cut objective for the true cluster is:", true_cluster_cut)
print("The cut objective for the random cluster is:", random_cluster_cut)
```
The cut objective for the true cluster is: 0.0

The cut objective for the random cluster is: 383.0

From the result above, it shows that this part of the cut objective indeed favors the true clusters over the random ones.

### B.2 The Volume Term 

> In this part, I'll write a function called `vols(A,y)` which computes the volumes of <img src="https://render.githubusercontent.com/render/math?math=C_0"> and <img src="https://render.githubusercontent.com/render/math?math=C_1">, returning them as a tuple. (for example, `v0, v1 = vols(A,y)`)

Now take a look at the second factor in the norm cut objective. This is the *volume term*. As mentioned above, the *volume* of cluster <img src="https://render.githubusercontent.com/render/math?math=C_0"> is a measure of how "big" cluster <img src="https://render.githubusercontent.com/render/math?math=C_0"> is. If we choose cluster <img src="https://render.githubusercontent.com/render/math?math=C_0"> to be small, then <img src="https://render.githubusercontent.com/render/math?math=\mathbf{vol}(C_0)"> will be small and <img src="https://render.githubusercontent.com/render/math?math=\frac{1}{\mathbf{vol}(C_0)}"> will be large, leading to an undesirable higher objective value. 

Synthesizing, the binary normcut objective asks us to find clusters <img src="https://render.githubusercontent.com/render/math?math=C_0"> and <img src="https://render.githubusercontent.com/render/math?math=C_1"> such that:

1. There are relatively few entries of <img src="https://render.githubusercontent.com/render/math?math=\mathbf{A}"> that join <img src="https://render.githubusercontent.com/render/math?math=C_0"> and <img src="https://render.githubusercontent.com/render/math?math=C_1">. 
2. Neither <img src="https://render.githubusercontent.com/render/math?math=C_0"> and <img src="https://render.githubusercontent.com/render/math?math=C_1"> are too small. 

Below is the function def of `vols(A, y)`:

```python
def vols(A,y):
  """
  compute the volume of cluster C0 and C1 of
  the similarity matrix A specified by y

  Input
  --------
  A: the similarity matrix created
  y: the cluster-specifying vector, 0 or 1 valued entries
  
  Output
  --------
  (C0_vol, C1_vol): the volumes of clusters C0 and C1
  """
  # create a matrix d to store row sums of A
  d = A.sum(axis = 0)
  # find C0_vol and C1_vol by indexing sums
  C0_vol = d[y == 0].sum()
  C1_vol = d[y == 1].sum()
  # return output
  return (C0_vol, C1_vol)
```

{::options parse_block_html="true" /}
<div class="got-help">

I changed the original for-loop with list comprehension version into the direct application to the np.sum function, which is suggested from my peer's feedback.

Version before change:

```python
def vols(A,y):
  """
  compute the volume of cluster C0 and C1 of
  the similarity matrix A specified by y

  Input
  --------
  A: the similarity matrix created
  y: the cluster-specifying vector, 0 or 1 valued entries
  
  Output
  --------
  (C0_vol, C1_vol): the volumes of clusters C0 and C1
  """
  # create the two clusters
  C0 = np.where(y == 0)[0]
  C1 = np.where(y == 1)[0]
  # calculate volume of C0 and C1
  C0_vol = np.sum([np.sum(A[i, ]) for i in C0])
  C1_vol = np.sum([np.sum(A[j, ]) for j in C1])
  # return volumes
  return (C0_vol, C1_vol)
```

Peer's Feedback: "I believe that you could make your code for the function vols(A,y) more efficient. You could first calculate the d matrix, which contains the sums for each row of the matrix A (you could use np.sum for this). Then, you could use this d to find this sums for cluster C0 and C1 using boolean indexing (something like v0 = d[y==0].sum() ). In this way, you could avoid for loops and increase your efficiency :)."
</div>
{::options parse_block_html="false" /}

### B.3 The Norm Cut Calculation

> In this part, I'll write a function called `normcut(A,y)`, which uses `cut(A,y)` and `vols(A,y)` to compute the binary normalized cut objective of a matrix **A** with clustering vector `y`. 

Based on equationed definition mentioned in *part B.0*, can calculate the Norm Cut.

Below is the function def of `normcut(A, y)`:

```python
def normcut(A, y):
  """
  calculate the normcut of cluster C0 and C1 of
  the similarity matrix A specified by y

  Input
  --------
  A: the similarity matrix created
  y: the cluster-specifying vector, 0 or 1 valued entries
  
  Output
  --------
  normcut: the normcut of clusters C0 and C1
  """
  vol_c0, vol_c1 = vols(A, y)
  normcut = cut(A, y) * (1 / vol_c0 + 1 / vol_c1)
  return normcut
```

### B.4 Norm Cut Comparison

Similar as *part B.2*, now compare the calculated norm cut objective on both *y* and *random* to conclude that the norm cut objective also favors the true clusters over the random ones.

```python
# cut with true labels y
true_labels_normcut = normcut(A, y)
# cut with fake labels vector
fake_labels_normcut = normcut(A, random)
# find the two values with print
print("The normcut objective for the true cluster is:", true_labels_normcut)
print("The normcut objective for the random cluster is:", fake_labels_normcut)
```
The normcut objective for the true cluster is: 0.0

The normcut objective for the random cluster is: 1.0074122463651651

From the result, the normcut objective for true cluster is 0, which is way lower than about 1 of the one for random cluster. Therefore, can conclude that the norm cut objective also favors the true clusters over the random ones.

## Part C: Find Small Normcut - The (D, z) Definition of Normcut

> This part helps to prove that the value `y` for making `normcut(A, y)` small can be calculated using a specified equation that could be calculated directly as follows:

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$


One approach to clustering is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem, which means that may not be possible to find the best clustering in practical time, even for relatively small data sets.

Here's a math trick to solve the problem: define a new vector <img src="https://render.githubusercontent.com/render/math?math=\mathbf{z} \in \mathbb{R}^n"> such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$


Note that the signs of  the elements of <img src="https://render.githubusercontent.com/render/math?math=\mathbf{z}"> contain all the information from <img src="https://render.githubusercontent.com/render/math?math=\mathbf{y}">: if `i` is in cluster <img src="https://render.githubusercontent.com/render/math?math=C_0">, then <img src="https://render.githubusercontent.com/render/math?math=y_i = 0"> and <img src="https://render.githubusercontent.com/render/math?math=z_i > 0">. 

Next, if you like linear algebra, you can show that 

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $\mathbf{D}$ is the diagonal matrix with nonzero entries <img src="https://render.githubusercontent.com/render/math?math=">$d_{ii} = d_i$, and  where <img src="https://render.githubusercontent.com/render/math?math=">$d_i = \sum_{j = 1}^n a_i$ is the degree (row-sum) from before.

Now, let's do the computation process, in the following part, I'll:

1. Write a function called `transform(A,y)`: compute the appropriate <img src="https://render.githubusercontent.com/render/math?math=\mathbf{z}"> vector given **A** and **y**, using the formula above.

```python
def transform(A,y):
  """
  calculate z-vector of cluster C0 and C1 of
  the similarity matrix A specified by y

  Input
  --------
  A: the similarity matrix created
  y: the cluster-specifying vector, 0 or 1 valued entries
  
  Output
  --------
  z: the z-vector of clusters C0 and C1
  """
  z = np.zeros(y.shape[0])
  vol_c0, vol_c1 = vols(A, y)
  z[y == 0] = 1 / vol_c0
  z[y == 1] = -1 / vol_c1
  return z
```

2. Construct the matrix D from A

```python
def D_construction(A):
  """
  calculate the desired diagonal rowsum-valued matrix D from A
  """
  # create vector storing all rowsums of sim matrix A
  A_row_sum = [np.sum(A[i, ]) for i in np.arange(A.shape[0])]
  # construct and return result D
  D = np.diag(A_row_sum)
  return D
```

3. Show that the equation (want to prove) holds

Here the equation above is exact, but computer arithmetic is not! The python function `np.isclose(a,b)` is a good way to check if `a` is "close" to `b`, in the sense that they differ by less than the smallest amount that the computer is (by default) able to quantify.

Below is the total construction process use functions in 1 and 2 as well as checking whether the equation holds (LHS = RHS):

```python
# calculate the diagonal rowsum-valued matrix D
D = D_construction(A)
# calculate z-vector
z = transform(A, y)
# find the right hand side of the equation
normcut_with_D_z = 2 * z@(D-A)@z / (z@D@z)
# check validility and correctness of the equation
# note: normcut(A, y) calculate left hand side of the eqn
np.isclose(normcut(A, y), normcut_with_D_z)
```

## Part D: Minimize Normcut Objective

As shown in last part, the normcut function

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$


Then, the normcut objective is now the mathematical optimization problem of minimizing the function:

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$


<img src="https://render.githubusercontent.com/render/math?math=">
subject to the condition <img src="https://render.githubusercontent.com/render/math?math=\mathbf{z}^T\mathbf{D}\mathbb{1} = 0">. It's actually possible to bake this condition into the optimization, by substituting for <img src="https://render.githubusercontent.com/render/math?math=\mathbf{z}"> the orthogonal complement of <img src="https://render.githubusercontent.com/render/math?math=\mathbf{z}"> relative to <img src="https://render.githubusercontent.com/render/math?math=\mathbf{D}\mathbf{1}">. Below function `orth_obj` function handles this porblem. 

```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```

Now, use the function `minimize` from `scipy.optimize` module to minimize the function `orth_obj` with respect to <img src="https://render.githubusercontent.com/render/math?math=\mathbf{z}">.

```python
from scipy.optimize import minimize

minimized_contents = minimize(orth_obj, z)
minimized_contents.keys()
```
dict_keys(['fun', 'jac', 'hess_inv', 'nfev', 'njev', 'status', 'success', 'message', 'x', 'nit'])

Now, can see that the minimized contents contains a lot of components, the desired data result we want is actually from the content `'x'`. Therefore, store the minimized `z` as `z_min` with the `'x'` key from `minimized_contents`:

```python
z_min = minimized_contents.x
```

## Part E: Scattering Spectral Clusters with Minimized Normcut

From `part d`, get that `z_min` is the clustering that minimize the Normcut. Now, plot the scatter plot by changing coloring to `z_min`:

```python
plt.scatter(X[:,0], X[:,1], c = z_min)
```
![BP2_part_e.png]({{ site.baseurl }}/images/BP2_part_e.png)

Nice! With the spectral clustering, now the scatters are with the correct clusters based on `z_min`.

## Part F: Implicit Optimization of Orthogonal Objectives

From part d, we actually do the explicit optimization using the function `minimize` supported by python. However, it is way too complicated sometimes when programming. Actually, the minimization problem can be solved using eigenvalues and eigenvectors of matrices. Below is an explanation on how things works:

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

<img src="https://render.githubusercontent.com/render/math?math=">

with respect to <img src="https://render.githubusercontent.com/render/math?math=\mathbf{z}">, subject to the condition <img src="https://render.githubusercontent.com/render/math?math=\mathbf{z}^T\mathbf{D}\mathbb{1} = 0">. 

The Rayleigh-Ritz Theorem states that the minimizing <img src="https://render.githubusercontent.com/render/math?math=\mathbf{z}"> must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

where <img src="https://render.githubusercontent.com/render/math?math=\mathbb{1}"> is actually the eigenvector with smallest eigenvalue of the matrix <img src="https://render.githubusercontent.com/render/math?math=\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})">. 

Therefore, the vector <img src="https://render.githubusercontent.com/render/math?math=\mathbf{z}"> that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

Based on the above explanation, now we can construct the optimization process in python:

1. Construct the matrix <img src="https://render.githubusercontent.com/render/math?math=\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})">, which is often called the (normalized) *Laplacian* matrix of the similarity matrix <img src="https://render.githubusercontent.com/render/math?math=\mathbf{A}">.

To construct the Laplacian matrix, I wrote a function names `L_construction`, which input a matrix A and output its Laplacian matrix. Below is the function def:

```python
def L_construction(A):
  """
  Construct a similarity matrix indicator for whether each X[i]
  and X[j] are less than epsilon apart. Also, diagonal entries
  should all be zero for the desired output matrix.

  Input
  --------
  X: all data points informations
  epsilon: the maximum distance to be accepted

  Output
  --------
  A: the desired similarity matrix
  """
  # create vector storing all rowsums of sim matrix A
  A_row_sum = [np.sum(A[i, ]) for i in np.arange(A.shape[0])]
  # create the diagonal rowsum-valued matrix D
  D = np.diag(A_row_sum)
  # calculate L
  L = np.linalg.inv(D)@(D - A)
  return L
```


2. Find the eigenvector corresponding to its second-smallest eigenvalue, and call it `z_eig`.

First, I wrote a function for finding the second smallest eigenvalue and its corresponding eigenvector. The function `second_smallest_eigen` inputs the desired Laplacian matrix, and outputs the second smallest eigenvalue and its corresponding eigenvector as a tuple. Below is the function construction:

```python
def second_smallest_eigen(L):
  """
  return second smallest eigenvalue and corresponding eigenvector
  """
  # Lam gives the eigenvalues of L, U gives the eigenvectors (as columns)
  Lam, U = np.linalg.eig(L)

  # eigenvector with second-smallest eigenvalue
  # spoiler, you're going to need this one soon!
  ix = Lam.argsort()
  Lam, U = Lam[ix], U[:,ix]

  # 2nd smallest eigenvalue and corresponding eigenvector
  return (Lam[1], U[:,1])
```

Now, with the above function as well as the `L_construction` function in 1, can calculate the optimized `z` based on this second smallest eigenvector, name it as `z_eig`. Below is the construction process of `z_eig`.

```python
# calculate L
L = L_construction(A)
# calculate 2nd smallest eigenvalue and corresponding eigenvector
Lam[1], U[:,1] = second_smallest_eigen(L)
# store the 2nd smallest-eigenvalued eigenvector as z_eig
z_eig = U[:,1]
```

3. Plot the data again, using the sign of `z_eig` as the color.

Now, with `z_eig`, can do the plotting process similar as *part e*:

```python
plt.scatter(X[:,0], X[:,1], c = z_eig)
```
![BP2_part_f.png]({{ site.baseurl }}/images/BP2_part_f.png)

And again, the scattering is correct!

## Part G: Spectral Clustering

> This part writes a function `spectral_clustering(X, epsilon)`, which basically do the whole spectral clustering process, which is a previous process based summary.

The function `spectral_clustering(X, epsilon)` inputs the data points information `X` and the given distance threshold `epsilon`, and outputs the spectral clustering labeling vector `labels`.

Below is the definition of this function:

```python
def spectral_clustering(X, epsilon):
  """
  Performs spectral clustering based on input data X 
  and the distance threshold epsilon, returning 
  an array of binary labels indicating whether
  data point i is in group 0 or group 1.

  The process is concatenated as four steps:
  1. Construct the similarity matrix.
  2. Construct the Laplacian matrix.
  3. Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix.
  4. Return labels based on this eigenvector.

  Input
  --------
  X: all data points informations
  epsilon: the maximum distance to be accepted

  Output
  --------
  labels: array of binary labels indicating whether data point i is in group 0 or 1
  """

  # Step 1: Construct the similarity matrix.
  A = similarity_matrix_construction(X, epsilon)
  # Step 2: Construct the Laplacian matrix.
  L = L_construction(A)
  # Step 3: Compute the eigenvector with second-smallest eigenvalue of L
  sec_small_eigvalue, sec_small_eigvec = second_smallest_eigen(L)
  # Step 4: Return labels based on this eigenvector.
  labels = sec_small_eigvec
  labels[sec_small_eigvec >= 0] = 1
  labels[sec_small_eigvec < 0] = 0
  return labels
```

Now, try to visualize the result labels to make sure the function works, recall that `epsilon = 0.4` is already defined:

```python
spectral_clustering(X, epsilon)
```

```text
array([1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1.,
       0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
       1., 1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.,
       0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0.,
       0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 1.,
       1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 1.,
       1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,
       1., 1., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 1.,
       1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,
       1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0.,
       1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 1., 0.,
       1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 0., 0., 0.])
```

Seems that the function works properly! Now we can work on the graph visualizations to see whether clustering are correctly defined.

## Part H: Mooned Clustering with Different Noise

> This part plots some mooned clustering plots based on different noise levels. To specify, this part fixes number of samples to 1000, and fix `epsilon` to 0.5

With the spectral_clustering function defined, can plot mooned clusters based on spectral clustering with different noise.

First, write a function to do the whole plotting process, with different inputted noise. Below is the function def:

```python
np.random.seed(9999) # set random seed
def plot_scatter_with_noise(noise):
  """
  plot based on different input noises
  """
  X, y = datasets.make_moons(n_samples=1000, shuffle=True, noise=noise, random_state=None)
  labels = spectral_clustering(X, epsilon = 0.5)
  plt.scatter(X[:,0], X[:,1], c = labels)
```

Now, plot with different noises:

1. Noise Level = 0.05

```python
plot_scatter_with_noise(0.05)
```
![BP2_part_h_0.05.png]({{ site.baseurl }}/images/BP2_part_h_0.05.png)

2. Noise Level = 0.10

```python
plot_scatter_with_noise(0.1)
```
![BP2_part_h_0.1.png]({{ site.baseurl }}/images/BP2_part_h_0.1.png)

3. Noise Level = 0.15

```python
plot_scatter_with_noise(0.15)
```
![BP2_part_h_0.15.png]({{ site.baseurl }}/images/BP2_part_h_0.15.png)

4. Noise Level = 0.20

```python
plot_scatter_with_noise(0.2)
```
![BP2_part_h_0.2.png]({{ site.baseurl }}/images/BP2_part_h_0.2.png)

As can see from the above plots, as the noise increases, the "diameter" of the moon increases, and the two moons are more likely to mix between each other. In other words, as noise increases, the general distances between two clusters are decreasing, making the clustering process harder (need lower epsilons).

## Part I: More Confirmation in Correctness - Clustering the Bull's Eye

Now let's see that the clustering process works on the bull's eye.

The bull's eye clusters is created and visualized as follows:

```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```
![BP2_part_i_circle.png]({{ site.baseurl }}/images/BP2_part_i_circle.png)

With comparison, first do the clusterning based on the K-Means method:

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
![BP2_part_i_km.png]({{ site.baseurl }}/images/BP2_part_i_km.png)

Clearly can see, the k-means methods does not correctly clustering the bull's eye.

Now let's try with spectral clustering method, with `epsilon = 0.6`:

```python
plt.scatter(X[:,0], X[:,1], c = spectral_clustering(X, epsilon = 0.6))
```
![BP2_part_i_sc.png]({{ site.baseurl }}/images/BP2_part_i_sc.png)

Yes! Our spectral clustering algorithm correctly clusters the bull's eyes

{::options parse_block_html="true" /}
<div class="got-help">
I learned something really cool from my peer feedback! 
</div>
{::options parse_block_html="false" /}

{::options parse_block_html="true" /}
<div class="gave-help">
I gave one of my peers a cool suggestion! 
</div>
{::options parse_block_html="false" /}