---
layout: post
title: Blog Post 2 - Spectral Clustering
---

> In this blog post, I'll write a tutorial on a simple version of the *spectral clustering* algorithm for clustering data points. Each of the below parts are necessary specific tasks for creating the spectral clustering.

### Notation

Before starting doing some linear algebra in python, here are some specified notations used in this blog post:

- Boldface capital letters like **A** refer to matrices (2d arrays of numbers). 
- Boldface lowercase letters like **v** refer to vectors (1d arrays of numbers). 
- **AB** refers to a matrix-matrix product (`A@B`). **Av** refers to a matrix-vector product (`A@v`).

## Introduction

This part is the introduction of studying *spectral clustering*. Spectral clustering is an important tool for identifying meaningful parts of data sets with complex structure. To start, let's look at an example where we *don't* need spectral clustering.

```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```
```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```
BP2_intro_1.png

*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these:

```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
BP2_intro_2.png


## Harder Clustering

That was all well and good, but what if our data is "shaped weird"? Here, we can use the function `make_moons` supported by python to make the moonlike clusters:

```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```
BP2_harder_clustering_1.png

Similar as in *Intro*, can still make out two meaningful clusters in the data, but now they aren't blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. 

```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```
BP2_harder_clustering_2.png

Clearly from the figure above, the clustering went wrong with the k-Means algorithm.

As will suggest below, spectral clustering is able to correctly cluster the two crescents. The following parts shows how to derive and implement spectral clustering.

## Part A: Similarity Matrix Construction

> In this part, construct a *similarity matrix* **A**, a matrix (2d `np.ndarray`) with shape `(n, n)` (recall that `n` is the number of data points), 

- The matrix construction should be based on another external parameter `epsilon`. Specifically, for each entry `A[i,j]` of **A**, the value should be equal to one if data point `i` is within the distance `epsilon` of data point j, which is to say,

$$A[i, j] = 1 \text{ if } (X[i] - X[j])**2 < epsilon**2$$

- Moreover, the diagonal entries `A[i,i]` should all be equal to zero.

For this part, I specified the value `epsilon = 0.4`. Also, to be more convenient for function construction, I used the function `pairwise_distance` from `sklearn.metrics` module, which could compute the euclidean distance between specified data points i and j.

```python
epsilon = 0.4
from sklearn.metrics import pairwise_distances
```

To construct the similarity matrix **A**, I construct a function `similarity_matrix_construction`, which inputs the set of data points `X` and the `epsilon` value. Below is the function def.

```python
def similarity_matrix_construction(X, epsilon):
  """
  Construct a similarity matrix indicator for whether each X[i]
  and X[j] are less than epsilon apart. Also, diagonal entries
  should all be zero for the desired output matrix.

  Input
  --------
  X: all data points informations
  epsilon: the maximum distance to be accepted

  Output
  --------
  A: the desired similarity matrix
  """
  # get dimension
  n = X.shape[0]
  # create empty matrix
  A = np.ndarray(shape = (n, n))
  # create matrix of distances using pairwise_distances
  # note: pairwise distances create a matrix D with D_ij as
  # distance between point X[i] and X[j]
  D = pairwise_distances(X, metric = 'euclidean')
  # now try to fill in A with information in D and epsilon
  A[D < epsilon**2] = 1
  A[D >= epsilon**2] = 0
  # empty the diagonal entries
  np.fill_diagonal(A, 0)
  # return similarity matrix
  return A
```

Now, store such similarity matrix as **A** for further use, and visualize the matrix:

```python
A = similarity_matrix_construction(X, epsilon)
A
```
array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 1.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 1., 0., 0.]])

The matrix **A** now contains information about which points are near (within distance `epsilon`) which other points.

## Part B: Clustering Data Points - Binary Norm Cut Objective of Similarity Matrix

> This part pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of **A**. 

### B.0 Introduce the Concept of Binary Norm Cut Objective

Let $d_i = \sum_{j = 1}^n a_{ij}$ be the $i$th row-sum of $\mathbf{A}$, which is also called the *degree* of $i$. Let $C_0$ and $C_1$ be two clusters of the data points. Assume that every data point is in either $C_0$ or $C_1$. The cluster membership as being specified by `y`. Think of `y[i]` as being the label of point `i`. So, if `y[i] = 1`, then point `i` (and therefore row $i$ of **A**) is an element of cluster $C_1$.  

The *binary norm cut objective* of a matrix **A** is the function 

$$N_A(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;$$

In this expression, 
- $\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$ is the *cut* of the clusters $C_0$ and $C_1$. 
- $\mathbf{vol}(C_0) \equiv \sum_{i \in C_0}d_i$, where $d_i = \sum_{j = 1}^n a_{ij}$ is the *degree* of row $i$ (the total number of all other rows related to row $i$ through $A$). The *volume* of cluster $C_0$ is a measure of the size of the cluster. 

A pair of clusters $C_0$ and $C_1$ is considered to be a "good" partition of the data when $N_{\mathbf{A}}(C_0, C_1)$ is small. To see why, let's look at each of the two factors in this objective function separately. 


### B.1 The Cut Term

First, the cut term $\mathbf{cut}(C_0, C_1)$ is the number of nonzero entries in $\mathbf{A}$ that relate points in cluster $C_0$ to points in cluster $C_1$. Saying that this term should be small is the same as saying that points in $C_0$ shouldn't usually be very close to points in $C_1$. 

Write a function called `cut(A,y)` to compute the cut term. You can compute it by summing up the entries `A[i,j]` for each pair of points `(i,j)` in different clusters. 

It's ok if you use `for`-loops in this function -- we are going to see a more efficient view of this problem soon. 










{::options parse_block_html="true" /}
<div class="got-help">
I learned something really cool from my peer feedback! 
</div>
{::options parse_block_html="false" /}

{::options parse_block_html="true" /}
<div class="gave-help">
I gave one of my peers a cool suggestion! 
</div>
{::options parse_block_html="false" /}